{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Setting the random seed\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (40pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One hot encode the labels\n",
    "def one_hot_encode(y):\n",
    "    enc = pd.get_dummies(y)\n",
    "    return np.matrix(enc)\n",
    "\n",
    "\n",
    "# Get the cost of iteration\n",
    "def get_cost(y, y_pred):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y))\n",
    "    return cost\n",
    "    \n",
    "\n",
    "#Get the accuracy of the results\n",
    "def get_accuracy(y, y_pred):\n",
    "    y = tf.nn.softmax(y)\n",
    "    y_pred = tf.nn.softmax(y_pred)\n",
    "    labels_equal = tf.equal(y, y_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(labels_equal, 'float'))\n",
    "    return accuracy\n",
    "\n",
    "# Initialize the hidden layer and output layer dimensions and apply activations on them\n",
    "def predict_deep_net(X, inputLayerSize, hiddenLayerSizes, outputLayerSize):\n",
    "    \n",
    "    # Random Weight Initialization\n",
    "    hidden_layer_1 = {'W': tf.get_variable('W1', [inputLayerSize, hiddenLayerSizes[0]], initializer=tf.contrib.layers.xavier_initializer(seed = 1)),\n",
    "                       'b': tf.get_variable('b1', [hiddenLayerSizes[0]],dtype=tf.float32, initializer=tf.zeros_initializer())}\n",
    "    \n",
    "    hidden_layer_2 = {'W':tf.get_variable('W2', [hiddenLayerSizes[0], hiddenLayerSizes[1]], initializer=tf.contrib.layers.xavier_initializer(seed = 1)),\n",
    "                       'b':tf.get_variable('b2', [hiddenLayerSizes[1]],dtype=tf.float32, initializer=tf.zeros_initializer())}\n",
    "    \n",
    "    output_layer = {'W':tf.get_variable('W3', [hiddenLayerSizes[1], outputLayerSize], initializer=tf.contrib.layers.xavier_initializer(seed = 1)),\n",
    "                       'b':tf.get_variable('b3', [outputLayerSize],dtype=tf.float32, initializer=tf.zeros_initializer())}\n",
    "    \n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(tf.cast(X, tf.float32),hidden_layer_1['W']), hidden_layer_1['b'])\n",
    "    layer1 = tf.nn.relu(layer1) # Relu on 1st Hidden Layer\n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(layer1,hidden_layer_2['W']), hidden_layer_2['b'])\n",
    "    layer2 = tf.nn.relu(layer2) # Relu on 2nd Hidden Layer\n",
    "\n",
    "    output = tf.matmul(layer2,output_layer['W']) + output_layer['b']\n",
    "    output = tf.nn.sigmoid(output) # Sigmoid on Output Layer\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "data_train = pd.read_csv('ex4_train.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_train = data_train.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_train = df_train['y']\n",
    "X_train = df_train.drop(['y'], axis=1)\n",
    "\n",
    "X_train_mat = np.matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25    7\n",
      "Name: y, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113aa6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the test data\n",
    "data_test = pd.read_csv('ex4_test.csv', sep=\",\", encoding='utf-8', header='infer')\n",
    "df_test = data_test.drop('Unnamed: 0',axis=1)\n",
    "m = df_test.shape[0]\n",
    "\n",
    "y_test = df_test['y']\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "X_test_mat = np.matrix(X_test)\n",
    "\n",
    "# Plot the selected pixel\n",
    "num = 25\n",
    "pixels = np.array(X_test[num:num+1], dtype='uint8')\n",
    "print(y_test[num:num+1])\n",
    "pixels = pixels.reshape((20, 20)).T\n",
    "plt.imshow(pixels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputLayerSize = X_train.shape[1]\n",
    "hiddenLayers = 2\n",
    "hiddenLayerSizes = [100,40]\n",
    "outputLayerSize = 10\n",
    "\n",
    "# One hot encode the labels\n",
    "y_train_mat = one_hot_encode(y_train)\n",
    "y_test_mat = one_hot_encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "1    8\n",
      "2    8\n",
      "3    0\n",
      "4    9\n",
      "Name: y, dtype: int64\n",
      "0    5\n",
      "1    7\n",
      "2    5\n",
      "3    2\n",
      "4    9\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.head())\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (30pts) Neural Network model with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-2-3d95343ba4c5>\", line 26, in predict_deep_net\n    hidden_layer_1 = {tf.get_variable('W', [inputLayerSize, hiddenLayerSizes[0]], initializer=tf.contrib.layers.xavier_initializer(seed = 1)),\n  File \"<ipython-input-7-45bbbfb3dd8a>\", line 4, in train_deep_neural_net\n    y_pred = predict_deep_net(x, inputLayerSize, hiddenLayerSizes, outputLayerSize)\n  File \"<ipython-input-7-45bbbfb3dd8a>\", line 21, in <module>\n    train_deep_neural_net(1000, X_train_mat, y_train_mat, X_test_mat, y_test_mat)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-45bbbfb3dd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_deep_neural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-45bbbfb3dd8a>\u001b[0m in \u001b[0;36mtrain_deep_neural_net\u001b[0;34m(epochs, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_deep_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputLayerSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddenLayerSizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputLayerSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# The AdamOptimizer is used in place of gradient descent to optimize the cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3d95343ba4c5>\u001b[0m in \u001b[0;36mpredict_deep_net\u001b[0;34m(X, inputLayerSize, hiddenLayerSizes, outputLayerSize)\u001b[0m\n\u001b[1;32m     27\u001b[0m                     tf.get_variable('b', [hiddenLayerSizes[0]],dtype=tf.float32, initializer=tf.zeros_initializer())}\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     hidden_layer_2 = {tf.get_variable('W', [hiddenLayerSizes[0], hiddenLayerSizes[1]], initializer=tf.contrib.layers.xavier_initializer(seed = 1)),\n\u001b[0m\u001b[1;32m     30\u001b[0m                        tf.get_variable('b', [hiddenLayerSizes[1]],dtype=tf.float32, initializer=tf.zeros_initializer())}\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-2-3d95343ba4c5>\", line 26, in predict_deep_net\n    hidden_layer_1 = {tf.get_variable('W', [inputLayerSize, hiddenLayerSizes[0]], initializer=tf.contrib.layers.xavier_initializer(seed = 1)),\n  File \"<ipython-input-7-45bbbfb3dd8a>\", line 4, in train_deep_neural_net\n    y_pred = predict_deep_net(x, inputLayerSize, hiddenLayerSizes, outputLayerSize)\n  File \"<ipython-input-7-45bbbfb3dd8a>\", line 21, in <module>\n    train_deep_neural_net(1000, X_train_mat, y_train_mat, X_test_mat, y_test_mat)\n"
     ]
    }
   ],
   "source": [
    "def train_deep_neural_net(epochs, X_train, y_train, X_test, y_test):\n",
    "    x = tf.placeholder(tf.float32, [None, X_train.shape[1]])\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    y_pred = predict_deep_net(x, inputLayerSize, hiddenLayerSizes, outputLayerSize)\n",
    "    cost = get_cost(y, y_pred)\n",
    "    # The AdamOptimizer is used in place of gradient descent to optimize the cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost) \n",
    "    init = tf.global_variables_initializer()\n",
    "    correctness = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, 'float'))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            temp, epoch_cost = sess.run([optimizer, cost], feed_dict={x:X_train, y:y_train})\n",
    "            if(epoch % 100 == 0):\n",
    "                print('Cost is ' + str(epoch_cost) + ' for iteration: ' + str(epoch))\n",
    "        print('Accuracy Train: ', accuracy.eval({x:X_train, y:y_train}))\n",
    "        print('Accuracy Test: ', accuracy.eval({x:X_test, y:y_test}))\n",
    "#         \n",
    "\n",
    "train_deep_neural_net(1000, X_train_mat, y_train_mat, X_test_mat, y_test_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
